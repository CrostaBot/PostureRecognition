{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1 x 1</th>\n",
       "      <th>1 x 2</th>\n",
       "      <th>1 x 3</th>\n",
       "      <th>1 x 4</th>\n",
       "      <th>1 x 5</th>\n",
       "      <th>1 x 6</th>\n",
       "      <th>1 x 7</th>\n",
       "      <th>1 x 8</th>\n",
       "      <th>1 x 9</th>\n",
       "      <th>1 x 10</th>\n",
       "      <th>...</th>\n",
       "      <th>64 x 25</th>\n",
       "      <th>64 x 26</th>\n",
       "      <th>64 x 27</th>\n",
       "      <th>64 x 28</th>\n",
       "      <th>64 x 29</th>\n",
       "      <th>64 x 30</th>\n",
       "      <th>64 x 31</th>\n",
       "      <th>64 x 32</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2.319124</td>\n",
       "      <td>0.427245</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.037231</td>\n",
       "      <td>0.017234</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.148059</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.425649</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.013218</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>6.915289</td>\n",
       "      <td>0.792687</td>\n",
       "      <td>0.426752</td>\n",
       "      <td>1.373588</td>\n",
       "      <td>1.090417</td>\n",
       "      <td>1.457864</td>\n",
       "      <td>0.039052</td>\n",
       "      <td>0.102649</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016591</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014987</td>\n",
       "      <td>0.035087</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.313572</td>\n",
       "      <td>0.966576</td>\n",
       "      <td>1.783004</td>\n",
       "      <td>3.543893</td>\n",
       "      <td>...</td>\n",
       "      <td>2.796575</td>\n",
       "      <td>0.291005</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.563542</td>\n",
       "      <td>2.079184</td>\n",
       "      <td>0.205778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.585625</td>\n",
       "      <td>1.219705</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028111</td>\n",
       "      <td>0.291441</td>\n",
       "      <td>0.261475</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.819086</td>\n",
       "      <td>0.250982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.068844</td>\n",
       "      <td>0.117745</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.157808</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>23.034799</td>\n",
       "      <td>5.318210</td>\n",
       "      <td>1.194308</td>\n",
       "      <td>0.555494</td>\n",
       "      <td>1.066160</td>\n",
       "      <td>0.016610</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.242610</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.467665</td>\n",
       "      <td>0.757722</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.940948</td>\n",
       "      <td>5.060682</td>\n",
       "      <td>0.532293</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.508528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006102</td>\n",
       "      <td>0.145598</td>\n",
       "      <td>0.158254</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.064535</td>\n",
       "      <td>0.317231</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.309126</td>\n",
       "      <td>0.215606</td>\n",
       "      <td>0.825949</td>\n",
       "      <td>0.374073</td>\n",
       "      <td>0.090734</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.458259</td>\n",
       "      <td>...</td>\n",
       "      <td>4.887065</td>\n",
       "      <td>0.921055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.107826</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.032191</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011720</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.015955</td>\n",
       "      <td>0.028164</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.021628</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.987641</td>\n",
       "      <td>1.162368</td>\n",
       "      <td>0.870081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.090298</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2672 rows × 2050 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      1 x 1     1 x 2  1 x 3     1 x 4     1 x 5     1 x 6     1 x 7  \\\n",
       "0       0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "1       0.0  0.000000    0.0  0.425649  0.000000  0.013218  0.000000   \n",
       "2       0.0  0.016591    0.0  0.014987  0.035087  0.000000  0.313572   \n",
       "3       0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "4       0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "...     ...       ...    ...       ...       ...       ...       ...   \n",
       "2667    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "2668    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "2669    0.0  0.508528    0.0  0.000000  0.006102  0.145598  0.158254   \n",
       "2670    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.000000   \n",
       "2671    0.0  0.000000    0.0  0.000000  0.000000  0.000000  0.011720   \n",
       "\n",
       "         1 x 8     1 x 9    1 x 10  ...    64 x 25   64 x 26   64 x 27  \\\n",
       "0     0.000000  0.000000  0.000000  ...   2.319124  0.427245  0.000000   \n",
       "1     0.000000  0.000000  0.000000  ...   6.915289  0.792687  0.426752   \n",
       "2     0.966576  1.783004  3.543893  ...   2.796575  0.291005  0.000000   \n",
       "3     0.000000  0.000000  0.000000  ...   0.585625  1.219705  0.000000   \n",
       "4     0.000000  0.000000  0.000000  ...   0.819086  0.250982  0.000000   \n",
       "...        ...       ...       ...  ...        ...       ...       ...   \n",
       "2667  0.000000  0.000000  0.000000  ...  23.034799  5.318210  1.194308   \n",
       "2668  0.000000  0.000000  0.000000  ...  12.467665  0.757722  0.000000   \n",
       "2669  0.123435  0.064535  0.317231  ...   0.000000  0.309126  0.215606   \n",
       "2670  0.000000  0.000000  0.458259  ...   4.887065  0.921055  0.000000   \n",
       "2671  0.000000  0.015955  0.028164  ...   0.000000  1.021628  0.000000   \n",
       "\n",
       "       64 x 28   64 x 29   64 x 30   64 x 31   64 x 32  Subject  Position  \n",
       "0     0.000000  1.037231  0.017234  0.000000  0.148059        4         0  \n",
       "1     1.373588  1.090417  1.457864  0.039052  0.102649        0         2  \n",
       "2     0.563542  2.079184  0.205778  0.000000  0.000000        5         0  \n",
       "3     0.028111  0.291441  0.261475  0.000000  0.000000        6         0  \n",
       "4     0.068844  0.117745  0.000000  0.000000  0.157808        3         0  \n",
       "...        ...       ...       ...       ...       ...      ...       ...  \n",
       "2667  0.555494  1.066160  0.016610  0.000000  0.242610        5         2  \n",
       "2668  1.940948  5.060682  0.532293  0.000000  0.000000        7         2  \n",
       "2669  0.825949  0.374073  0.090734  0.000000  0.000000        5         2  \n",
       "2670  1.107826  0.000704  0.032191  0.000000  0.000000        7         1  \n",
       "2671  0.987641  1.162368  0.870081  0.000000  0.090298       10         0  \n",
       "\n",
       "[2672 rows x 2050 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset and shuffle data\n",
    "df = pd.read_csv('dataset.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# Split dataset in train, val and test\n",
    "num_col = len(df.columns)\n",
    "train_split = 2400\n",
    "test_split = 2600\n",
    "\n",
    "train_data = df.iloc[0:train_split, 0:num_col-2]\n",
    "val_data = df.iloc[train_split:test_split, 0:num_col-2]\n",
    "\n",
    "train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "test_data = df.iloc[test_split:,:num_col-2]\n",
    "test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "test_data = np.array(test_data)\n",
    "test_subject = np.array(test_subject)\n",
    "test_position = np.array(test_position)\n",
    "test_data = test_data.reshape(len(test_data), 64,32, 1)\n",
    "test_data = test_data / 500\n",
    "test_subject = to_categorical(test_subject)\n",
    "test_position = to_categorical(test_position)\n",
    "\n",
    "print(test_subject.shape[1])\n",
    "\n",
    "while test_subject.shape[1] != 13:\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    train_data = df.iloc[0:train_split, 0:num_col-2]\n",
    "    val_data = df.iloc[train_split:test_split, 0:num_col-2]\n",
    "\n",
    "    train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "    train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "    val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "    val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "    test_data = df.iloc[test_split:,:num_col-2]\n",
    "    test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "    test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "    test_data = np.array(test_data)\n",
    "    test_subject = np.array(test_subject)\n",
    "    test_position = np.array(test_position)\n",
    "    test_data = test_data.reshape(len(test_data), 64,32, 1)\n",
    "    test_data = test_data / 500\n",
    "    test_subject = to_categorical(test_subject)\n",
    "    test_position = to_categorical(test_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 64, 32, 1)\n",
      "(2400, 1)\n",
      "(2400, 1)\n",
      "(200, 64, 32, 1)\n",
      "(200, 1)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "val_data = np.array(val_data)\n",
    "\n",
    "train_subject = np.array(train_subject)\n",
    "train_position = np.array(train_position)\n",
    "val_subject = np.array(val_subject)\n",
    "val_position = np.array(val_position)\n",
    "\n",
    "train_data = train_data.reshape(len(train_data), 64,32, 1)\n",
    "val_data = val_data.reshape(len(val_data), 64,32, 1)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_subject.shape)\n",
    "print(train_position.shape)\n",
    "print(val_data.shape)\n",
    "print(val_subject.shape)\n",
    "print(val_position.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "i = 0\n",
    "\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    \n",
    "    label = str(train_subject[i-1]) + ' - ' + str(train_position[i-1])\n",
    "    plt.imshow(train_data[i-1].reshape((64, 32)), cmap='gray')\n",
    "    plt.xlabel(label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data and build categorical labels\n",
    "train_data = train_data / 500\n",
    "val_data = val_data / 500\n",
    "\n",
    "train_subject = to_categorical(train_subject, 13)\n",
    "train_position = to_categorical(train_position, 3)\n",
    "val_subject = to_categorical(val_subject, 13)\n",
    "val_position = to_categorical(val_position, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = Input((64, 32, 1),)\n",
    "\n",
    "x = Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(64, 32, 1))(inp)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "    \n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='valid')(x)\n",
    "x = MaxPooling2D((2, 2))(x)\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "p = Flatten()(x)\n",
    "p = Dense(120, activation='relu')(p)\n",
    "p = Dense(84, activation='relu')(p)\n",
    "\n",
    "p = Dropout(0.4)(p)\n",
    "\n",
    "position = Dense(3, activation='softmax')(p)\n",
    "\n",
    "s = Conv2D(16, (3, 3), activation='relu', padding='valid')(x)\n",
    "s = MaxPooling2D((2, 2))(s)\n",
    "s = Dropout(0.4)(s)\n",
    "\n",
    "s = Flatten()(s)\n",
    "s = Dense(120, activation='relu')(s)\n",
    "s = Dense(84, activation='relu')(s)\n",
    "\n",
    "s = Dropout(0.4)(s)\n",
    "\n",
    "subject = Dense(13, activation='softmax')(s)\n",
    "\n",
    "model_subject = Model(inp, subject)\n",
    "model_position = Model(inp, position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 13, 5, 16)         2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 6, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 6, 2, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 192)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 120)               23160     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 13)                1105      \n",
      "=================================================================\n",
      "Total params: 41,693\n",
      "Trainable params: 41,693\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 64, 32, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 64, 32, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 32, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 14, 16)        4624      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 7, 16)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1680)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 120)               201720    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 84)                10164     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 84)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 255       \n",
      "=================================================================\n",
      "Total params: 217,083\n",
      "Trainable params: 217,083\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_subject.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_position.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_subject.summary()\n",
    "model_position.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 2.5416 - accuracy: 0.0954 - val_loss: 2.5283 - val_accuracy: 0.1600\n",
      "Epoch 2/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 2.4505 - accuracy: 0.1433 - val_loss: 2.3265 - val_accuracy: 0.2650\n",
      "Epoch 3/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 2.3132 - accuracy: 0.1929 - val_loss: 2.1018 - val_accuracy: 0.2500\n",
      "Epoch 4/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 2.1613 - accuracy: 0.2567 - val_loss: 1.9705 - val_accuracy: 0.3050\n",
      "Epoch 5/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 1.9785 - accuracy: 0.3208 - val_loss: 1.6231 - val_accuracy: 0.4650\n",
      "Epoch 6/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 1.8189 - accuracy: 0.3792 - val_loss: 1.5072 - val_accuracy: 0.5300\n",
      "Epoch 7/200\n",
      "300/300 [==============================] - 6s 18ms/step - loss: 1.6586 - accuracy: 0.4171 - val_loss: 1.2185 - val_accuracy: 0.6450\n",
      "Epoch 8/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 1.5589 - accuracy: 0.4696 - val_loss: 1.1608 - val_accuracy: 0.7000\n",
      "Epoch 9/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 1.4284 - accuracy: 0.5079 - val_loss: 0.9735 - val_accuracy: 0.7600\n",
      "Epoch 10/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 1.3341 - accuracy: 0.5462 - val_loss: 0.9046 - val_accuracy: 0.7750\n",
      "Epoch 11/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 1.2817 - accuracy: 0.5700 - val_loss: 0.8091 - val_accuracy: 0.7900\n",
      "Epoch 12/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 1.1909 - accuracy: 0.6046 - val_loss: 0.7418 - val_accuracy: 0.8050\n",
      "Epoch 13/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 1.1311 - accuracy: 0.6142 - val_loss: 0.7032 - val_accuracy: 0.8200\n",
      "Epoch 14/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 1.0909 - accuracy: 0.6292 - val_loss: 0.6788 - val_accuracy: 0.8100\n",
      "Epoch 15/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 1.0190 - accuracy: 0.6562 - val_loss: 0.6462 - val_accuracy: 0.8200\n",
      "Epoch 16/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.9896 - accuracy: 0.6658 - val_loss: 0.6294 - val_accuracy: 0.8150\n",
      "Epoch 17/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.9229 - accuracy: 0.6900 - val_loss: 0.6042 - val_accuracy: 0.8350\n",
      "Epoch 18/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.8902 - accuracy: 0.7013 - val_loss: 0.5664 - val_accuracy: 0.8350\n",
      "Epoch 19/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.8871 - accuracy: 0.7008 - val_loss: 0.5582 - val_accuracy: 0.8400\n",
      "Epoch 20/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 0.8392 - accuracy: 0.7175 - val_loss: 0.5474 - val_accuracy: 0.8350\n",
      "Epoch 21/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.7794 - accuracy: 0.7337 - val_loss: 0.5107 - val_accuracy: 0.8300\n",
      "Epoch 22/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.8380 - accuracy: 0.7179 - val_loss: 0.5292 - val_accuracy: 0.8550\n",
      "Epoch 23/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.7805 - accuracy: 0.7375 - val_loss: 0.4985 - val_accuracy: 0.8450\n",
      "Epoch 24/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.7523 - accuracy: 0.7467 - val_loss: 0.5112 - val_accuracy: 0.8450\n",
      "Epoch 25/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.7386 - accuracy: 0.7579 - val_loss: 0.4930 - val_accuracy: 0.8500\n",
      "Epoch 26/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 0.7153 - accuracy: 0.7546 - val_loss: 0.4789 - val_accuracy: 0.8750\n",
      "Epoch 27/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.6894 - accuracy: 0.7729 - val_loss: 0.4397 - val_accuracy: 0.8650\n",
      "Epoch 28/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.6549 - accuracy: 0.7804 - val_loss: 0.4544 - val_accuracy: 0.8800\n",
      "Epoch 29/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.6500 - accuracy: 0.7842 - val_loss: 0.4864 - val_accuracy: 0.8500\n",
      "Epoch 30/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.6638 - accuracy: 0.7875 - val_loss: 0.4602 - val_accuracy: 0.8650\n",
      "Epoch 31/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.6403 - accuracy: 0.7812 - val_loss: 0.4549 - val_accuracy: 0.8600\n",
      "Epoch 32/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.6142 - accuracy: 0.7896 - val_loss: 0.4648 - val_accuracy: 0.8650\n",
      "Epoch 33/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.6223 - accuracy: 0.7979 - val_loss: 0.4489 - val_accuracy: 0.8750\n",
      "Epoch 34/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.5670 - accuracy: 0.8100 - val_loss: 0.4601 - val_accuracy: 0.8500\n",
      "Epoch 35/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 0.5582 - accuracy: 0.8087 - val_loss: 0.4754 - val_accuracy: 0.8500\n",
      "Epoch 36/200\n",
      "300/300 [==============================] - 5s 18ms/step - loss: 0.5525 - accuracy: 0.8138 - val_loss: 0.4624 - val_accuracy: 0.8450\n",
      "Epoch 37/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5726 - accuracy: 0.8042 - val_loss: 0.4532 - val_accuracy: 0.8450\n",
      "Epoch 38/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5740 - accuracy: 0.8092 - val_loss: 0.4436 - val_accuracy: 0.8350\n",
      "Epoch 39/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5292 - accuracy: 0.8271 - val_loss: 0.4770 - val_accuracy: 0.8450\n",
      "Epoch 40/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5314 - accuracy: 0.8175 - val_loss: 0.4622 - val_accuracy: 0.8450\n",
      "Epoch 41/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5436 - accuracy: 0.8192 - val_loss: 0.4520 - val_accuracy: 0.8500\n",
      "Epoch 42/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5371 - accuracy: 0.8096 - val_loss: 0.4978 - val_accuracy: 0.8400\n",
      "Epoch 43/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.5535 - accuracy: 0.8221 - val_loss: 0.4679 - val_accuracy: 0.8300\n",
      "Epoch 44/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.5256 - accuracy: 0.8238 - val_loss: 0.4387 - val_accuracy: 0.8450\n",
      "Epoch 45/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.5162 - accuracy: 0.8283 - val_loss: 0.4542 - val_accuracy: 0.8450\n",
      "Epoch 46/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4486 - accuracy: 0.8546 - val_loss: 0.4831 - val_accuracy: 0.8550\n",
      "Epoch 47/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4790 - accuracy: 0.8354 - val_loss: 0.4439 - val_accuracy: 0.8450\n",
      "Epoch 48/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4958 - accuracy: 0.8342 - val_loss: 0.4747 - val_accuracy: 0.8500\n",
      "Epoch 49/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4548 - accuracy: 0.8471 - val_loss: 0.4077 - val_accuracy: 0.8700\n",
      "Epoch 50/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4769 - accuracy: 0.8413 - val_loss: 0.4051 - val_accuracy: 0.8700\n",
      "Epoch 51/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4771 - accuracy: 0.8363 - val_loss: 0.4771 - val_accuracy: 0.8550\n",
      "Epoch 52/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4542 - accuracy: 0.8329 - val_loss: 0.4701 - val_accuracy: 0.8650\n",
      "Epoch 53/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4648 - accuracy: 0.8433 - val_loss: 0.5172 - val_accuracy: 0.8300\n",
      "Epoch 54/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4508 - accuracy: 0.8404 - val_loss: 0.4523 - val_accuracy: 0.8600\n",
      "Epoch 55/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.4670 - accuracy: 0.8433 - val_loss: 0.4765 - val_accuracy: 0.8650\n",
      "Epoch 56/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4500 - accuracy: 0.8462 - val_loss: 0.4596 - val_accuracy: 0.8600\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4625 - accuracy: 0.8354 - val_loss: 0.4519 - val_accuracy: 0.8400\n",
      "Epoch 58/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4575 - accuracy: 0.8454 - val_loss: 0.4419 - val_accuracy: 0.8750\n",
      "Epoch 59/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4526 - accuracy: 0.8508 - val_loss: 0.4396 - val_accuracy: 0.8650\n",
      "Epoch 60/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4293 - accuracy: 0.8504 - val_loss: 0.4363 - val_accuracy: 0.8500\n",
      "Epoch 61/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4421 - accuracy: 0.8542 - val_loss: 0.4786 - val_accuracy: 0.8650\n",
      "Epoch 62/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4373 - accuracy: 0.8533 - val_loss: 0.4577 - val_accuracy: 0.8650\n",
      "Epoch 63/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4092 - accuracy: 0.8629 - val_loss: 0.4193 - val_accuracy: 0.8400\n",
      "Epoch 64/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4285 - accuracy: 0.8533 - val_loss: 0.4337 - val_accuracy: 0.8650\n",
      "Epoch 65/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.4159 - accuracy: 0.8562 - val_loss: 0.4508 - val_accuracy: 0.8650\n",
      "Epoch 66/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.4242 - accuracy: 0.8479 - val_loss: 0.4366 - val_accuracy: 0.8400\n",
      "Epoch 67/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3985 - accuracy: 0.8654 - val_loss: 0.4458 - val_accuracy: 0.8650\n",
      "Epoch 68/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4357 - accuracy: 0.8596 - val_loss: 0.4142 - val_accuracy: 0.8650\n",
      "Epoch 69/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3915 - accuracy: 0.8650 - val_loss: 0.4459 - val_accuracy: 0.8500\n",
      "Epoch 70/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4380 - accuracy: 0.8521 - val_loss: 0.4125 - val_accuracy: 0.8600\n",
      "Epoch 71/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3924 - accuracy: 0.8800 - val_loss: 0.4121 - val_accuracy: 0.8550\n",
      "Epoch 72/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3981 - accuracy: 0.8621 - val_loss: 0.4399 - val_accuracy: 0.8650\n",
      "Epoch 73/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4292 - accuracy: 0.8508 - val_loss: 0.4338 - val_accuracy: 0.8700\n",
      "Epoch 74/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4054 - accuracy: 0.8629 - val_loss: 0.4431 - val_accuracy: 0.8600\n",
      "Epoch 75/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3774 - accuracy: 0.8687 - val_loss: 0.4694 - val_accuracy: 0.8700\n",
      "Epoch 76/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4073 - accuracy: 0.8629 - val_loss: 0.4568 - val_accuracy: 0.8750\n",
      "Epoch 77/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4059 - accuracy: 0.8621 - val_loss: 0.4664 - val_accuracy: 0.8600\n",
      "Epoch 78/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4122 - accuracy: 0.8575 - val_loss: 0.4495 - val_accuracy: 0.8650\n",
      "Epoch 79/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.4000 - accuracy: 0.8721 - val_loss: 0.4313 - val_accuracy: 0.8950\n",
      "Epoch 80/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3915 - accuracy: 0.8671 - val_loss: 0.4543 - val_accuracy: 0.8500\n",
      "Epoch 81/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3868 - accuracy: 0.8683 - val_loss: 0.4622 - val_accuracy: 0.8650\n",
      "Epoch 82/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3788 - accuracy: 0.8675 - val_loss: 0.4529 - val_accuracy: 0.8600\n",
      "Epoch 83/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3639 - accuracy: 0.8671 - val_loss: 0.4949 - val_accuracy: 0.8550\n",
      "Epoch 84/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3443 - accuracy: 0.8767 - val_loss: 0.4653 - val_accuracy: 0.8800\n",
      "Epoch 85/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3938 - accuracy: 0.8587 - val_loss: 0.4600 - val_accuracy: 0.8650\n",
      "Epoch 86/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3893 - accuracy: 0.8671 - val_loss: 0.4725 - val_accuracy: 0.8700\n",
      "Epoch 87/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3695 - accuracy: 0.8717 - val_loss: 0.4775 - val_accuracy: 0.8650\n",
      "Epoch 88/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3753 - accuracy: 0.8679 - val_loss: 0.5209 - val_accuracy: 0.8550\n",
      "Epoch 89/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3694 - accuracy: 0.8675 - val_loss: 0.4881 - val_accuracy: 0.8650\n",
      "Epoch 90/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3640 - accuracy: 0.8758 - val_loss: 0.4963 - val_accuracy: 0.8550\n",
      "Epoch 91/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3707 - accuracy: 0.8733 - val_loss: 0.5087 - val_accuracy: 0.8700\n",
      "Epoch 92/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3348 - accuracy: 0.8783 - val_loss: 0.4491 - val_accuracy: 0.8850\n",
      "Epoch 93/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3456 - accuracy: 0.8779 - val_loss: 0.4792 - val_accuracy: 0.8750\n",
      "Epoch 94/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3567 - accuracy: 0.8763 - val_loss: 0.4826 - val_accuracy: 0.8600\n",
      "Epoch 95/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3440 - accuracy: 0.8758 - val_loss: 0.4790 - val_accuracy: 0.8600\n",
      "Epoch 96/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3951 - accuracy: 0.8667 - val_loss: 0.4706 - val_accuracy: 0.8450\n",
      "Epoch 97/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3527 - accuracy: 0.8767 - val_loss: 0.4960 - val_accuracy: 0.8800\n",
      "Epoch 98/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3590 - accuracy: 0.8813 - val_loss: 0.4816 - val_accuracy: 0.8800\n",
      "Epoch 99/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3470 - accuracy: 0.8854 - val_loss: 0.4369 - val_accuracy: 0.8650\n",
      "Epoch 100/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3743 - accuracy: 0.8717 - val_loss: 0.4353 - val_accuracy: 0.8800\n",
      "Epoch 101/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3830 - accuracy: 0.8633 - val_loss: 0.4502 - val_accuracy: 0.8700\n",
      "Epoch 102/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3522 - accuracy: 0.8737 - val_loss: 0.4397 - val_accuracy: 0.8900\n",
      "Epoch 103/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3531 - accuracy: 0.8813 - val_loss: 0.4528 - val_accuracy: 0.8600\n",
      "Epoch 104/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3558 - accuracy: 0.8775 - val_loss: 0.4318 - val_accuracy: 0.8700\n",
      "Epoch 105/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3398 - accuracy: 0.8854 - val_loss: 0.4637 - val_accuracy: 0.8650\n",
      "Epoch 106/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3255 - accuracy: 0.8921 - val_loss: 0.4728 - val_accuracy: 0.8700\n",
      "Epoch 107/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3202 - accuracy: 0.8925 - val_loss: 0.5171 - val_accuracy: 0.8550\n",
      "Epoch 108/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3347 - accuracy: 0.8825 - val_loss: 0.4681 - val_accuracy: 0.8550\n",
      "Epoch 109/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3439 - accuracy: 0.8883 - val_loss: 0.4877 - val_accuracy: 0.8800\n",
      "Epoch 110/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3313 - accuracy: 0.8825 - val_loss: 0.4743 - val_accuracy: 0.8750\n",
      "Epoch 111/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3452 - accuracy: 0.8829 - val_loss: 0.4452 - val_accuracy: 0.8800\n",
      "Epoch 112/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3249 - accuracy: 0.8863 - val_loss: 0.4268 - val_accuracy: 0.8850\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3376 - accuracy: 0.8867 - val_loss: 0.5024 - val_accuracy: 0.8700\n",
      "Epoch 114/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3493 - accuracy: 0.8717 - val_loss: 0.4847 - val_accuracy: 0.8750\n",
      "Epoch 115/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3168 - accuracy: 0.8925 - val_loss: 0.4767 - val_accuracy: 0.9000\n",
      "Epoch 116/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3284 - accuracy: 0.8896 - val_loss: 0.4994 - val_accuracy: 0.8650\n",
      "Epoch 117/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2879 - accuracy: 0.8938 - val_loss: 0.5149 - val_accuracy: 0.8800\n",
      "Epoch 118/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3146 - accuracy: 0.8875 - val_loss: 0.5108 - val_accuracy: 0.8650\n",
      "Epoch 119/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3265 - accuracy: 0.8817 - val_loss: 0.5198 - val_accuracy: 0.8650\n",
      "Epoch 120/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3337 - accuracy: 0.8892 - val_loss: 0.4980 - val_accuracy: 0.8650\n",
      "Epoch 121/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3513 - accuracy: 0.8771 - val_loss: 0.4793 - val_accuracy: 0.8650\n",
      "Epoch 122/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3143 - accuracy: 0.8908 - val_loss: 0.4233 - val_accuracy: 0.8800\n",
      "Epoch 123/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2905 - accuracy: 0.8917 - val_loss: 0.4564 - val_accuracy: 0.8700\n",
      "Epoch 124/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3366 - accuracy: 0.8833 - val_loss: 0.4721 - val_accuracy: 0.8800\n",
      "Epoch 125/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3123 - accuracy: 0.8967 - val_loss: 0.4663 - val_accuracy: 0.8800\n",
      "Epoch 126/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3481 - accuracy: 0.8788 - val_loss: 0.4681 - val_accuracy: 0.8850\n",
      "Epoch 127/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3309 - accuracy: 0.8858 - val_loss: 0.4274 - val_accuracy: 0.8550\n",
      "Epoch 128/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3202 - accuracy: 0.8908 - val_loss: 0.4396 - val_accuracy: 0.8600\n",
      "Epoch 129/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3068 - accuracy: 0.8875 - val_loss: 0.4543 - val_accuracy: 0.8700\n",
      "Epoch 130/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3035 - accuracy: 0.8896 - val_loss: 0.4119 - val_accuracy: 0.8800\n",
      "Epoch 131/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3069 - accuracy: 0.8913 - val_loss: 0.4315 - val_accuracy: 0.8600\n",
      "Epoch 132/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3260 - accuracy: 0.8904 - val_loss: 0.4531 - val_accuracy: 0.8900\n",
      "Epoch 133/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3369 - accuracy: 0.8833 - val_loss: 0.4485 - val_accuracy: 0.8600\n",
      "Epoch 134/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3025 - accuracy: 0.8950 - val_loss: 0.4452 - val_accuracy: 0.8650\n",
      "Epoch 135/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3303 - accuracy: 0.8846 - val_loss: 0.4707 - val_accuracy: 0.8800\n",
      "Epoch 136/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3205 - accuracy: 0.8888 - val_loss: 0.4228 - val_accuracy: 0.8650\n",
      "Epoch 137/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3331 - accuracy: 0.8896 - val_loss: 0.4313 - val_accuracy: 0.8700\n",
      "Epoch 138/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3186 - accuracy: 0.8942 - val_loss: 0.4618 - val_accuracy: 0.8600\n",
      "Epoch 139/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3228 - accuracy: 0.8879 - val_loss: 0.4598 - val_accuracy: 0.8650\n",
      "Epoch 140/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2743 - accuracy: 0.9062 - val_loss: 0.4209 - val_accuracy: 0.8650\n",
      "Epoch 141/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.2878 - accuracy: 0.8942 - val_loss: 0.4569 - val_accuracy: 0.8800\n",
      "Epoch 142/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3182 - accuracy: 0.8975 - val_loss: 0.4744 - val_accuracy: 0.8650\n",
      "Epoch 143/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3087 - accuracy: 0.8983 - val_loss: 0.4596 - val_accuracy: 0.8850\n",
      "Epoch 144/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2972 - accuracy: 0.8925 - val_loss: 0.4622 - val_accuracy: 0.8900\n",
      "Epoch 145/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3005 - accuracy: 0.8938 - val_loss: 0.4762 - val_accuracy: 0.8600\n",
      "Epoch 146/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3162 - accuracy: 0.8975 - val_loss: 0.4633 - val_accuracy: 0.8800\n",
      "Epoch 147/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3104 - accuracy: 0.8908 - val_loss: 0.4703 - val_accuracy: 0.8950\n",
      "Epoch 148/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3232 - accuracy: 0.8879 - val_loss: 0.4650 - val_accuracy: 0.8700\n",
      "Epoch 149/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2815 - accuracy: 0.9025 - val_loss: 0.4773 - val_accuracy: 0.8650\n",
      "Epoch 150/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2949 - accuracy: 0.8979 - val_loss: 0.4342 - val_accuracy: 0.8750\n",
      "Epoch 151/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3149 - accuracy: 0.8967 - val_loss: 0.4198 - val_accuracy: 0.8550\n",
      "Epoch 152/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2914 - accuracy: 0.8983 - val_loss: 0.4460 - val_accuracy: 0.8500\n",
      "Epoch 153/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2768 - accuracy: 0.9000 - val_loss: 0.4031 - val_accuracy: 0.8750\n",
      "Epoch 154/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3093 - accuracy: 0.8883 - val_loss: 0.4380 - val_accuracy: 0.8800\n",
      "Epoch 155/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2868 - accuracy: 0.8988 - val_loss: 0.4317 - val_accuracy: 0.8700\n",
      "Epoch 156/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2952 - accuracy: 0.9075 - val_loss: 0.4657 - val_accuracy: 0.8650\n",
      "Epoch 157/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.3154 - accuracy: 0.8992 - val_loss: 0.4695 - val_accuracy: 0.8750\n",
      "Epoch 158/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.2890 - accuracy: 0.8979 - val_loss: 0.4531 - val_accuracy: 0.8850\n",
      "Epoch 159/200\n",
      "300/300 [==============================] - 4s 12ms/step - loss: 0.3110 - accuracy: 0.8942 - val_loss: 0.4255 - val_accuracy: 0.8700\n",
      "Epoch 160/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2806 - accuracy: 0.9067 - val_loss: 0.4370 - val_accuracy: 0.8700\n",
      "Epoch 161/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.3118 - accuracy: 0.8900 - val_loss: 0.4167 - val_accuracy: 0.8750\n",
      "Epoch 162/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.3019 - accuracy: 0.8992 - val_loss: 0.4170 - val_accuracy: 0.8850\n",
      "Epoch 163/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 0.2873 - accuracy: 0.8971 - val_loss: 0.4470 - val_accuracy: 0.8800\n",
      "Epoch 164/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.2850 - accuracy: 0.9013 - val_loss: 0.4442 - val_accuracy: 0.8750\n",
      "Epoch 165/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2980 - accuracy: 0.9008 - val_loss: 0.4336 - val_accuracy: 0.8750\n",
      "Epoch 166/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2924 - accuracy: 0.9004 - val_loss: 0.3935 - val_accuracy: 0.8750\n",
      "Epoch 167/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 0.3193 - accuracy: 0.9029 - val_loss: 0.4294 - val_accuracy: 0.8550\n",
      "Epoch 168/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2918 - accuracy: 0.9013 - val_loss: 0.4152 - val_accuracy: 0.8600\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2982 - accuracy: 0.9000 - val_loss: 0.4336 - val_accuracy: 0.8650\n",
      "Epoch 170/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2888 - accuracy: 0.9021 - val_loss: 0.4195 - val_accuracy: 0.8700\n",
      "Epoch 171/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2792 - accuracy: 0.8963 - val_loss: 0.4289 - val_accuracy: 0.8650\n",
      "Epoch 172/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2932 - accuracy: 0.8996 - val_loss: 0.4433 - val_accuracy: 0.8750\n",
      "Epoch 173/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.3019 - accuracy: 0.9062 - val_loss: 0.4462 - val_accuracy: 0.8750\n",
      "Epoch 174/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2858 - accuracy: 0.8971 - val_loss: 0.4350 - val_accuracy: 0.8650\n",
      "Epoch 175/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2659 - accuracy: 0.9025 - val_loss: 0.4588 - val_accuracy: 0.8600\n",
      "Epoch 176/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2928 - accuracy: 0.8971 - val_loss: 0.4202 - val_accuracy: 0.8700\n",
      "Epoch 177/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.2815 - accuracy: 0.8992 - val_loss: 0.4128 - val_accuracy: 0.8700\n",
      "Epoch 178/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2945 - accuracy: 0.8971 - val_loss: 0.3948 - val_accuracy: 0.8750\n",
      "Epoch 179/200\n",
      "300/300 [==============================] - 5s 17ms/step - loss: 0.2982 - accuracy: 0.8996 - val_loss: 0.3937 - val_accuracy: 0.8750\n",
      "Epoch 180/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 0.3002 - accuracy: 0.8996 - val_loss: 0.3926 - val_accuracy: 0.8550\n",
      "Epoch 181/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2837 - accuracy: 0.9050 - val_loss: 0.4344 - val_accuracy: 0.8500\n",
      "Epoch 182/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2616 - accuracy: 0.9058 - val_loss: 0.4316 - val_accuracy: 0.8750\n",
      "Epoch 183/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2843 - accuracy: 0.8996 - val_loss: 0.3775 - val_accuracy: 0.8750\n",
      "Epoch 184/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2623 - accuracy: 0.9050 - val_loss: 0.4170 - val_accuracy: 0.8600\n",
      "Epoch 185/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2982 - accuracy: 0.8983 - val_loss: 0.3939 - val_accuracy: 0.8850\n",
      "Epoch 186/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2693 - accuracy: 0.9021 - val_loss: 0.4005 - val_accuracy: 0.8750\n",
      "Epoch 187/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.3148 - accuracy: 0.8925 - val_loss: 0.3837 - val_accuracy: 0.8700\n",
      "Epoch 188/200\n",
      "300/300 [==============================] - 6s 20ms/step - loss: 0.2729 - accuracy: 0.9058 - val_loss: 0.3909 - val_accuracy: 0.8800\n",
      "Epoch 189/200\n",
      "300/300 [==============================] - 6s 19ms/step - loss: 0.2756 - accuracy: 0.9042 - val_loss: 0.4031 - val_accuracy: 0.8750\n",
      "Epoch 190/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2957 - accuracy: 0.8975 - val_loss: 0.4019 - val_accuracy: 0.8800\n",
      "Epoch 191/200\n",
      "300/300 [==============================] - 5s 15ms/step - loss: 0.2849 - accuracy: 0.9021 - val_loss: 0.4084 - val_accuracy: 0.8850\n",
      "Epoch 192/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2842 - accuracy: 0.9042 - val_loss: 0.3852 - val_accuracy: 0.8850\n",
      "Epoch 193/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2655 - accuracy: 0.9092 - val_loss: 0.4307 - val_accuracy: 0.8700\n",
      "Epoch 194/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2846 - accuracy: 0.9054 - val_loss: 0.4404 - val_accuracy: 0.8650\n",
      "Epoch 195/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.2813 - accuracy: 0.9033 - val_loss: 0.4522 - val_accuracy: 0.8600\n",
      "Epoch 196/200\n",
      "300/300 [==============================] - 5s 16ms/step - loss: 0.2855 - accuracy: 0.9100 - val_loss: 0.4659 - val_accuracy: 0.8550\n",
      "Epoch 197/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2821 - accuracy: 0.9054 - val_loss: 0.4231 - val_accuracy: 0.8650\n",
      "Epoch 198/200\n",
      "300/300 [==============================] - 4s 15ms/step - loss: 0.2881 - accuracy: 0.9004 - val_loss: 0.3858 - val_accuracy: 0.8800\n",
      "Epoch 199/200\n",
      "300/300 [==============================] - 4s 13ms/step - loss: 0.2913 - accuracy: 0.8954 - val_loss: 0.3992 - val_accuracy: 0.8750\n",
      "Epoch 200/200\n",
      "300/300 [==============================] - 4s 14ms/step - loss: 0.2435 - accuracy: 0.9137 - val_loss: 0.4283 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "#train subject model\n",
    "history = History()\n",
    "\n",
    "train_subject = model_subject.fit(train_data, train_subject, validation_data = (val_data, val_subject), epochs=200,batch_size = 8, callbacks = [history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ee6ed5c55409>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"--\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Validation accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy Curves'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(history.history['accuracy'],'black', linewidth = 3.0,  label = \"Training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'],'black', ls = \"--\",linewidth = 3.0, label = \"Validation accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train position model\n",
    "history = History()\n",
    "\n",
    "train_position = model_position.fit(train_data, train_position, validation_data = (val_data, val_position), epochs=10, callbacks = [history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(history.history['accuracy'],'black', linewidth = 3.0,  label = \"Training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'],'black', ls = \"--\",linewidth = 3.0, label = \"Validation accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_subject.evaluate(test_data, test_subject)\n",
    "model_position.evaluate(test_data, test_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_subject = model_subject.predict(test_data)\n",
    "predictions_position = model_position.predict(test_data)\n",
    "\n",
    "predictions_subject = predictions_subject.argmax(axis=-1)\n",
    "predictions_position = predictions_position.argmax(axis=-1)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "\n",
    "for i in range(1, 11):\n",
    "    plt.subplot(2,5,i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    \n",
    "    label = str(predictions_subject[i-1]) + ' - ' + str(predictions_position[i-1])\n",
    "    plt.imshow(test_data[i-1].reshape(64, 32), cmap='gray')\n",
    "    plt.xlabel(label)\n",
    "\n",
    "test_labels = df.iloc[test_split:, num_col-2:num_col]\n",
    "test_labels.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_position = model_position.predict(test_data)\n",
    "predictions_position = predictions_position.argmax(axis=-1)\n",
    "\n",
    "labels = ['supine', 'right', 'left']\n",
    "test_labels_p = df.iloc[test_split:, num_col-1:num_col]\n",
    "cm = confusion_matrix(test_labels_p, predictions_position, [0,1,2])\n",
    "\n",
    "f,ax= plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, ax = ax, linewidths=1, fmt = 'd', cmap = \"Blues\")\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Position'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_subject = model_subject.predict(test_data)\n",
    "predictions_subject = predictions_subject.argmax(axis=-1)\n",
    "\n",
    "labels = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6','S7', 'S8', 'S9','S10', 'S11', 'S12','S13']\n",
    "test_labels_s = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "cm = confusion_matrix(test_labels_s, predictions_subject, [0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "\n",
    "f,ax= plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, ax = ax, linewidths=1, fmt = 'd', cmap=\"Greens\")\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Subject'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and architecture to single file\n",
    "model_subject.save(\"Model/model_subject.h5\")\n",
    "model_position.save(\"Model/model_position.h5\")\n",
    "\n",
    "with open('Model/test.pkl', 'wb') as f:  # Python 3: open(..., 'wb')\n",
    "    pickle.dump([test_data, test_subject, test_position, test_labels_s, test_labels_p], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
