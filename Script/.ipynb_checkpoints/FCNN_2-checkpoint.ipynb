{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from tensorflow.keras.callbacks import History\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, Input\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Head X</th>\n",
       "      <th>Head Y</th>\n",
       "      <th>Head Pressure</th>\n",
       "      <th>Hip Bone X</th>\n",
       "      <th>Hip Bone Y</th>\n",
       "      <th>Hip Bone Pressure</th>\n",
       "      <th>Legs X</th>\n",
       "      <th>Legs Y</th>\n",
       "      <th>Legs Pressure</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21.945652</td>\n",
       "      <td>9.065217</td>\n",
       "      <td>0.240869</td>\n",
       "      <td>20.909091</td>\n",
       "      <td>24.190909</td>\n",
       "      <td>0.387038</td>\n",
       "      <td>23.122807</td>\n",
       "      <td>47.789474</td>\n",
       "      <td>0.122254</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.835106</td>\n",
       "      <td>7.505319</td>\n",
       "      <td>0.169367</td>\n",
       "      <td>14.191011</td>\n",
       "      <td>21.983146</td>\n",
       "      <td>0.201990</td>\n",
       "      <td>13.846154</td>\n",
       "      <td>41.961538</td>\n",
       "      <td>0.398611</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.322034</td>\n",
       "      <td>7.084746</td>\n",
       "      <td>0.107480</td>\n",
       "      <td>15.850427</td>\n",
       "      <td>25.085470</td>\n",
       "      <td>0.077687</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.575221</td>\n",
       "      <td>6.553097</td>\n",
       "      <td>0.019883</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.013991</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.945652</td>\n",
       "      <td>9.065217</td>\n",
       "      <td>0.240992</td>\n",
       "      <td>20.830189</td>\n",
       "      <td>24.311321</td>\n",
       "      <td>0.394344</td>\n",
       "      <td>23.122807</td>\n",
       "      <td>47.789474</td>\n",
       "      <td>0.122773</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>16.103448</td>\n",
       "      <td>10.120690</td>\n",
       "      <td>0.057873</td>\n",
       "      <td>16.977778</td>\n",
       "      <td>27.066667</td>\n",
       "      <td>0.099295</td>\n",
       "      <td>14.500000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>0.803004</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>17.147368</td>\n",
       "      <td>9.778947</td>\n",
       "      <td>0.109681</td>\n",
       "      <td>17.929293</td>\n",
       "      <td>29.898990</td>\n",
       "      <td>0.244188</td>\n",
       "      <td>15.490741</td>\n",
       "      <td>44.398148</td>\n",
       "      <td>0.095614</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>7.300000</td>\n",
       "      <td>10.300000</td>\n",
       "      <td>0.247548</td>\n",
       "      <td>7.733333</td>\n",
       "      <td>28.200000</td>\n",
       "      <td>0.342359</td>\n",
       "      <td>11.088889</td>\n",
       "      <td>47.200000</td>\n",
       "      <td>0.132186</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>18.260870</td>\n",
       "      <td>8.934783</td>\n",
       "      <td>0.307711</td>\n",
       "      <td>16.246154</td>\n",
       "      <td>25.584615</td>\n",
       "      <td>0.364235</td>\n",
       "      <td>15.426829</td>\n",
       "      <td>42.878049</td>\n",
       "      <td>0.095474</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2671</th>\n",
       "      <td>7.164319</td>\n",
       "      <td>6.784038</td>\n",
       "      <td>0.015291</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.702842</td>\n",
       "      <td>50.382429</td>\n",
       "      <td>0.016786</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2672 rows Ã— 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Head X     Head Y  Head Pressure  Hip Bone X  Hip Bone Y  \\\n",
       "0     21.945652   9.065217       0.240869   20.909091   24.190909   \n",
       "1     14.835106   7.505319       0.169367   14.191011   21.983146   \n",
       "2     19.322034   7.084746       0.107480   15.850427   25.085470   \n",
       "3      7.575221   6.553097       0.019883   14.000000   33.000000   \n",
       "4     21.945652   9.065217       0.240992   20.830189   24.311321   \n",
       "...         ...        ...            ...         ...         ...   \n",
       "2667  16.103448  10.120690       0.057873   16.977778   27.066667   \n",
       "2668  17.147368   9.778947       0.109681   17.929293   29.898990   \n",
       "2669   7.300000  10.300000       0.247548    7.733333   28.200000   \n",
       "2670  18.260870   8.934783       0.307711   16.246154   25.584615   \n",
       "2671   7.164319   6.784038       0.015291    0.000000   19.000000   \n",
       "\n",
       "      Hip Bone Pressure     Legs X     Legs Y  Legs Pressure  Subject  \\\n",
       "0              0.387038  23.122807  47.789474       0.122254        2   \n",
       "1              0.201990  13.846154  41.961538       0.398611        0   \n",
       "2              0.077687  14.000000  61.000000       1.000000        6   \n",
       "3              1.000000   7.500000  50.000000       0.013991        4   \n",
       "4              0.394344  23.122807  47.789474       0.122773        2   \n",
       "...                 ...        ...        ...            ...      ...   \n",
       "2667           0.099295  14.500000  61.000000       0.803004       11   \n",
       "2668           0.244188  15.490741  44.398148       0.095614        1   \n",
       "2669           0.342359  11.088889  47.200000       0.132186        7   \n",
       "2670           0.364235  15.426829  42.878049       0.095474       12   \n",
       "2671           1.000000   6.702842  50.382429       0.016786        6   \n",
       "\n",
       "      Position  \n",
       "0            2  \n",
       "1            0  \n",
       "2            2  \n",
       "3            0  \n",
       "4            2  \n",
       "...        ...  \n",
       "2667         0  \n",
       "2668         0  \n",
       "2669         1  \n",
       "2670         2  \n",
       "2671         1  \n",
       "\n",
       "[2672 rows x 11 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset and shuffle data\n",
    "df = pd.read_csv('dataset_2.csv')\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['Head Pressure'] = df['Head Pressure'].div(500)\n",
    "df['Hip Bone Pressure'] = df['Hip Bone Pressure'].div(500)\n",
    "df['Legs Pressure'] = df['Legs Pressure'].div(500)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in train, val and test\n",
    "num_col = len(df.columns)\n",
    "train_split = 2400\n",
    "test_split = 2600\n",
    "\n",
    "train_data = df.iloc[0:train_split, 0:num_col-2]\n",
    "val_data = df.iloc[train_split:test_split, 0:num_col-2]\n",
    "\n",
    "train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "test_data_p = df.iloc[test_split:,:num_col-2]\n",
    "test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "test_data_p = np.array(test_data_p)\n",
    "test_subject = np.array(test_subject)\n",
    "test_position = np.array(test_position)\n",
    "\n",
    "test_subject = to_categorical(test_subject)\n",
    "test_position = to_categorical(test_position)\n",
    "\n",
    "while test_subject.shape[1] != 13:\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    train_data = df.iloc[0:train_split, 0:num_col-2]\n",
    "    val_data = df.iloc[train_split:test_split, 0:num_col-2]\n",
    "\n",
    "    train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "    train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "    val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "    val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "    test_data_p = df.iloc[test_split:,:num_col-2]\n",
    "    test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "    test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "    test_data_p = np.array(test_data_p)\n",
    "    test_subject = np.array(test_subject)\n",
    "    test_position = np.array(test_position)\n",
    "    test_subject = to_categorical(test_subject)\n",
    "    test_position = to_categorical(test_position)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 9)\n",
      "(2400, 1)\n",
      "(2400, 1)\n",
      "(200, 9)\n",
      "(200, 1)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "val_data = np.array(val_data)\n",
    "\n",
    "train_subject = np.array(train_subject)\n",
    "train_position = np.array(train_position)\n",
    "val_subject = np.array(val_subject)\n",
    "val_position = np.array(val_position)\n",
    "\n",
    "print(train_data.shape)\n",
    "print(train_subject.shape)\n",
    "print(train_position.shape)\n",
    "print(val_data.shape)\n",
    "print(val_subject.shape)\n",
    "print(val_position.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "i = 0\n",
    "\n",
    "for i in range(1, 6):\n",
    "    plt.subplot(1,5,i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    \n",
    "    label = str(train_subject[i-1]) + ' - ' + str(train_position[i-1])\n",
    "    sns.heatmap(np.zeros(2048).reshape(64,32),  vmin=-2, cbar=False)\n",
    "    plt.plot(train_data[i][0],train_data[i][1], 'o')\n",
    "    plt.plot(train_data[i][3],train_data[i][4], 'o')\n",
    "    plt.plot(train_data[i][6],train_data[i][7], 'o')\n",
    "    plt.xlabel(label)\n",
    "\n",
    "plt.tight_layout()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data and build categorical labels\n",
    "train_subject = to_categorical(train_subject, 13)\n",
    "train_position = to_categorical(train_position, 3)\n",
    "val_subject = to_categorical(val_subject, 13)\n",
    "val_position = to_categorical(val_position, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model implementation\n",
    "inp = 9\n",
    "\n",
    "model_position = Sequential()\n",
    "model_position.add(Dense(64, input_dim= inp, activation='relu'))\n",
    "model_position.add(Flatten())\n",
    "model_position.add(Dropout(0.4))\n",
    "model_position.add(Dense(32, activation='relu'))\n",
    "model_position.add(Dense(32, activation='relu'))\n",
    "model_position.add(Dense(16, activation='relu'))\n",
    "model_position.add(Dense(9, activation='relu'))\n",
    "\n",
    "model_position.add(Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                640       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 9)                 153       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 3)                 30        \n",
      "=================================================================\n",
      "Total params: 4,487\n",
      "Trainable params: 4,487\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_position.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_position.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/250\n",
      "75/75 [==============================] - 1s 9ms/step - loss: 1.0809 - accuracy: 0.4800 - val_loss: 0.9741 - val_accuracy: 0.5200\n",
      "Epoch 2/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.9715 - accuracy: 0.5546 - val_loss: 0.9238 - val_accuracy: 0.5250\n",
      "Epoch 3/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.9029 - accuracy: 0.5804 - val_loss: 0.7994 - val_accuracy: 0.6900\n",
      "Epoch 4/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.8091 - accuracy: 0.6533 - val_loss: 0.7737 - val_accuracy: 0.7050\n",
      "Epoch 5/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.7296 - accuracy: 0.7079 - val_loss: 0.7438 - val_accuracy: 0.7200\n",
      "Epoch 6/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6905 - accuracy: 0.7304 - val_loss: 0.7195 - val_accuracy: 0.7500\n",
      "Epoch 7/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6351 - accuracy: 0.7642 - val_loss: 0.6625 - val_accuracy: 0.7850\n",
      "Epoch 8/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.6002 - accuracy: 0.7767 - val_loss: 0.6516 - val_accuracy: 0.7550\n",
      "Epoch 9/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5790 - accuracy: 0.7750 - val_loss: 0.6386 - val_accuracy: 0.7900\n",
      "Epoch 10/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.5697 - accuracy: 0.7792 - val_loss: 0.6195 - val_accuracy: 0.7950\n",
      "Epoch 11/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5331 - accuracy: 0.7992 - val_loss: 0.5828 - val_accuracy: 0.8100\n",
      "Epoch 12/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5278 - accuracy: 0.8037 - val_loss: 0.5902 - val_accuracy: 0.8000\n",
      "Epoch 13/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5266 - accuracy: 0.8100 - val_loss: 0.5805 - val_accuracy: 0.7850\n",
      "Epoch 14/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5053 - accuracy: 0.8250 - val_loss: 0.6023 - val_accuracy: 0.8250\n",
      "Epoch 15/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4990 - accuracy: 0.8267 - val_loss: 0.5488 - val_accuracy: 0.8300\n",
      "Epoch 16/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.5050 - accuracy: 0.8342 - val_loss: 0.5561 - val_accuracy: 0.8100\n",
      "Epoch 17/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.4586 - accuracy: 0.8479 - val_loss: 0.5605 - val_accuracy: 0.8250\n",
      "Epoch 18/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4810 - accuracy: 0.8333 - val_loss: 0.5208 - val_accuracy: 0.8450\n",
      "Epoch 19/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4607 - accuracy: 0.8442 - val_loss: 0.5159 - val_accuracy: 0.8450\n",
      "Epoch 20/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4508 - accuracy: 0.8500 - val_loss: 0.5382 - val_accuracy: 0.8450\n",
      "Epoch 21/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4275 - accuracy: 0.8646 - val_loss: 0.5496 - val_accuracy: 0.8200\n",
      "Epoch 22/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4402 - accuracy: 0.8525 - val_loss: 0.5371 - val_accuracy: 0.8500\n",
      "Epoch 23/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4367 - accuracy: 0.8529 - val_loss: 0.5148 - val_accuracy: 0.8500\n",
      "Epoch 24/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4329 - accuracy: 0.8558 - val_loss: 0.5277 - val_accuracy: 0.8450\n",
      "Epoch 25/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.4237 - accuracy: 0.8571 - val_loss: 0.4880 - val_accuracy: 0.8650\n",
      "Epoch 26/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4198 - accuracy: 0.8637 - val_loss: 0.5068 - val_accuracy: 0.8650\n",
      "Epoch 27/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4136 - accuracy: 0.8737 - val_loss: 0.4865 - val_accuracy: 0.8700\n",
      "Epoch 28/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3987 - accuracy: 0.8692 - val_loss: 0.4873 - val_accuracy: 0.8750\n",
      "Epoch 29/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.4050 - accuracy: 0.8721 - val_loss: 0.4927 - val_accuracy: 0.8550\n",
      "Epoch 30/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3871 - accuracy: 0.8742 - val_loss: 0.4543 - val_accuracy: 0.8550\n",
      "Epoch 31/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3847 - accuracy: 0.8763 - val_loss: 0.4554 - val_accuracy: 0.8750\n",
      "Epoch 32/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3783 - accuracy: 0.8800 - val_loss: 0.4478 - val_accuracy: 0.8750\n",
      "Epoch 33/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3807 - accuracy: 0.8767 - val_loss: 0.4386 - val_accuracy: 0.8850\n",
      "Epoch 34/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8775 - val_loss: 0.4427 - val_accuracy: 0.8800\n",
      "Epoch 35/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3776 - accuracy: 0.8729 - val_loss: 0.4278 - val_accuracy: 0.8800\n",
      "Epoch 36/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.3701 - accuracy: 0.8771 - val_loss: 0.4175 - val_accuracy: 0.8700\n",
      "Epoch 37/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3786 - accuracy: 0.8733 - val_loss: 0.4379 - val_accuracy: 0.8800\n",
      "Epoch 38/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3694 - accuracy: 0.8783 - val_loss: 0.4206 - val_accuracy: 0.8700\n",
      "Epoch 39/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3454 - accuracy: 0.8892 - val_loss: 0.4172 - val_accuracy: 0.8850\n",
      "Epoch 40/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3437 - accuracy: 0.8921 - val_loss: 0.4022 - val_accuracy: 0.8850\n",
      "Epoch 41/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3530 - accuracy: 0.8829 - val_loss: 0.4059 - val_accuracy: 0.8850\n",
      "Epoch 42/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3485 - accuracy: 0.8854 - val_loss: 0.3902 - val_accuracy: 0.8750\n",
      "Epoch 43/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3532 - accuracy: 0.8833 - val_loss: 0.4015 - val_accuracy: 0.8900\n",
      "Epoch 44/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3544 - accuracy: 0.8804 - val_loss: 0.3863 - val_accuracy: 0.8950\n",
      "Epoch 45/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3487 - accuracy: 0.8858 - val_loss: 0.3880 - val_accuracy: 0.8850\n",
      "Epoch 46/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3496 - accuracy: 0.8821 - val_loss: 0.4162 - val_accuracy: 0.8700\n",
      "Epoch 47/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3356 - accuracy: 0.8833 - val_loss: 0.3708 - val_accuracy: 0.9000\n",
      "Epoch 48/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3400 - accuracy: 0.8879 - val_loss: 0.3577 - val_accuracy: 0.8900\n",
      "Epoch 49/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3379 - accuracy: 0.8867 - val_loss: 0.3753 - val_accuracy: 0.8850\n",
      "Epoch 50/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8954 - val_loss: 0.3623 - val_accuracy: 0.8800\n",
      "Epoch 51/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3322 - accuracy: 0.8863 - val_loss: 0.3839 - val_accuracy: 0.8650\n",
      "Epoch 52/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3346 - accuracy: 0.8829 - val_loss: 0.3920 - val_accuracy: 0.8800\n",
      "Epoch 53/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3031 - accuracy: 0.8963 - val_loss: 0.3788 - val_accuracy: 0.8800\n",
      "Epoch 54/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3177 - accuracy: 0.8938 - val_loss: 0.3472 - val_accuracy: 0.8900\n",
      "Epoch 55/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3097 - accuracy: 0.8967 - val_loss: 0.3674 - val_accuracy: 0.8900\n",
      "Epoch 56/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8921 - val_loss: 0.3430 - val_accuracy: 0.8800\n",
      "Epoch 57/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3168 - accuracy: 0.8942 - val_loss: 0.3503 - val_accuracy: 0.8700\n",
      "Epoch 58/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.3136 - accuracy: 0.8954 - val_loss: 0.3292 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3087 - accuracy: 0.8983 - val_loss: 0.3273 - val_accuracy: 0.9100\n",
      "Epoch 60/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3038 - accuracy: 0.8917 - val_loss: 0.3293 - val_accuracy: 0.8950\n",
      "Epoch 61/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2967 - accuracy: 0.9029 - val_loss: 0.3740 - val_accuracy: 0.8650\n",
      "Epoch 62/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3059 - accuracy: 0.8933 - val_loss: 0.3310 - val_accuracy: 0.8900\n",
      "Epoch 63/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3023 - accuracy: 0.8954 - val_loss: 0.3344 - val_accuracy: 0.8750\n",
      "Epoch 64/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2963 - accuracy: 0.8913 - val_loss: 0.3253 - val_accuracy: 0.8900\n",
      "Epoch 65/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2932 - accuracy: 0.9046 - val_loss: 0.3234 - val_accuracy: 0.8900\n",
      "Epoch 66/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2796 - accuracy: 0.9029 - val_loss: 0.3043 - val_accuracy: 0.8950\n",
      "Epoch 67/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.3021 - accuracy: 0.8875 - val_loss: 0.3124 - val_accuracy: 0.8850\n",
      "Epoch 68/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2948 - accuracy: 0.9029 - val_loss: 0.3159 - val_accuracy: 0.8850\n",
      "Epoch 69/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2952 - accuracy: 0.9008 - val_loss: 0.3126 - val_accuracy: 0.8950\n",
      "Epoch 70/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2885 - accuracy: 0.9000 - val_loss: 0.2807 - val_accuracy: 0.9050\n",
      "Epoch 71/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2740 - accuracy: 0.9071 - val_loss: 0.2807 - val_accuracy: 0.9150\n",
      "Epoch 72/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2779 - accuracy: 0.9079 - val_loss: 0.2893 - val_accuracy: 0.8850\n",
      "Epoch 73/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2817 - accuracy: 0.9067 - val_loss: 0.3013 - val_accuracy: 0.8800\n",
      "Epoch 74/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2872 - accuracy: 0.9004 - val_loss: 0.2887 - val_accuracy: 0.9000\n",
      "Epoch 75/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2856 - accuracy: 0.8979 - val_loss: 0.3134 - val_accuracy: 0.8950\n",
      "Epoch 76/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2746 - accuracy: 0.9038 - val_loss: 0.3067 - val_accuracy: 0.8950\n",
      "Epoch 77/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2805 - accuracy: 0.9029 - val_loss: 0.3333 - val_accuracy: 0.8850\n",
      "Epoch 78/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2804 - accuracy: 0.9038 - val_loss: 0.2788 - val_accuracy: 0.9050\n",
      "Epoch 79/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2852 - accuracy: 0.9008 - val_loss: 0.2934 - val_accuracy: 0.9050\n",
      "Epoch 80/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2778 - accuracy: 0.8992 - val_loss: 0.2844 - val_accuracy: 0.9000\n",
      "Epoch 81/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2625 - accuracy: 0.9146 - val_loss: 0.2998 - val_accuracy: 0.8950\n",
      "Epoch 82/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2671 - accuracy: 0.9038 - val_loss: 0.2652 - val_accuracy: 0.9200\n",
      "Epoch 83/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2698 - accuracy: 0.9067 - val_loss: 0.2870 - val_accuracy: 0.9050\n",
      "Epoch 84/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2727 - accuracy: 0.9062 - val_loss: 0.2840 - val_accuracy: 0.8950\n",
      "Epoch 85/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2580 - accuracy: 0.9092 - val_loss: 0.2930 - val_accuracy: 0.8950\n",
      "Epoch 86/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2855 - accuracy: 0.9033 - val_loss: 0.2662 - val_accuracy: 0.9050\n",
      "Epoch 87/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2696 - accuracy: 0.9046 - val_loss: 0.2865 - val_accuracy: 0.9050\n",
      "Epoch 88/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2695 - accuracy: 0.9104 - val_loss: 0.3238 - val_accuracy: 0.8950\n",
      "Epoch 89/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2770 - accuracy: 0.9021 - val_loss: 0.2696 - val_accuracy: 0.9150\n",
      "Epoch 90/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2728 - accuracy: 0.9033 - val_loss: 0.2876 - val_accuracy: 0.9050\n",
      "Epoch 91/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.9087 - val_loss: 0.2835 - val_accuracy: 0.8950\n",
      "Epoch 92/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2642 - accuracy: 0.9017 - val_loss: 0.2597 - val_accuracy: 0.9200\n",
      "Epoch 93/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2576 - accuracy: 0.9154 - val_loss: 0.2879 - val_accuracy: 0.9000\n",
      "Epoch 94/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2693 - accuracy: 0.8992 - val_loss: 0.2835 - val_accuracy: 0.8950\n",
      "Epoch 95/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2648 - accuracy: 0.9075 - val_loss: 0.3048 - val_accuracy: 0.9150\n",
      "Epoch 96/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2485 - accuracy: 0.9133 - val_loss: 0.2773 - val_accuracy: 0.9050\n",
      "Epoch 97/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2537 - accuracy: 0.9067 - val_loss: 0.2623 - val_accuracy: 0.9150\n",
      "Epoch 98/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2688 - accuracy: 0.9112 - val_loss: 0.2537 - val_accuracy: 0.9300\n",
      "Epoch 99/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2489 - accuracy: 0.9121 - val_loss: 0.2767 - val_accuracy: 0.9000\n",
      "Epoch 100/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2541 - accuracy: 0.9079 - val_loss: 0.2769 - val_accuracy: 0.9000\n",
      "Epoch 101/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2511 - accuracy: 0.9083 - val_loss: 0.2549 - val_accuracy: 0.9050\n",
      "Epoch 102/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2548 - accuracy: 0.9117 - val_loss: 0.2647 - val_accuracy: 0.9100\n",
      "Epoch 103/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.2615 - accuracy: 0.9042 - val_loss: 0.2514 - val_accuracy: 0.9150\n",
      "Epoch 104/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2741 - accuracy: 0.9008 - val_loss: 0.2634 - val_accuracy: 0.9100\n",
      "Epoch 105/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2357 - accuracy: 0.9212 - val_loss: 0.2435 - val_accuracy: 0.9200\n",
      "Epoch 106/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2395 - accuracy: 0.9179 - val_loss: 0.2653 - val_accuracy: 0.9200\n",
      "Epoch 107/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.9087 - val_loss: 0.2585 - val_accuracy: 0.9200\n",
      "Epoch 108/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2623 - accuracy: 0.9079 - val_loss: 0.2608 - val_accuracy: 0.9250\n",
      "Epoch 109/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9196 - val_loss: 0.2762 - val_accuracy: 0.9150\n",
      "Epoch 110/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2442 - accuracy: 0.9183 - val_loss: 0.2459 - val_accuracy: 0.9150\n",
      "Epoch 111/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2401 - accuracy: 0.9137 - val_loss: 0.2796 - val_accuracy: 0.9200\n",
      "Epoch 112/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2529 - accuracy: 0.9062 - val_loss: 0.2469 - val_accuracy: 0.9100\n",
      "Epoch 113/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2413 - accuracy: 0.9117 - val_loss: 0.2960 - val_accuracy: 0.8950\n",
      "Epoch 114/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2453 - accuracy: 0.9200 - val_loss: 0.2673 - val_accuracy: 0.9000\n",
      "Epoch 115/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2462 - accuracy: 0.9150 - val_loss: 0.2644 - val_accuracy: 0.9050\n",
      "Epoch 116/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2300 - accuracy: 0.9221 - val_loss: 0.2629 - val_accuracy: 0.9050\n",
      "Epoch 117/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2391 - accuracy: 0.9146 - val_loss: 0.2666 - val_accuracy: 0.9000\n",
      "Epoch 118/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2371 - accuracy: 0.9217 - val_loss: 0.2529 - val_accuracy: 0.9100\n",
      "Epoch 119/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2306 - accuracy: 0.9175 - val_loss: 0.2659 - val_accuracy: 0.9150\n",
      "Epoch 120/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2676 - accuracy: 0.9087 - val_loss: 0.2763 - val_accuracy: 0.9000\n",
      "Epoch 121/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2531 - accuracy: 0.9104 - val_loss: 0.2656 - val_accuracy: 0.9100\n",
      "Epoch 122/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2467 - accuracy: 0.9083 - val_loss: 0.2983 - val_accuracy: 0.9050\n",
      "Epoch 123/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2472 - accuracy: 0.9167 - val_loss: 0.2620 - val_accuracy: 0.9050\n",
      "Epoch 124/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2359 - accuracy: 0.9179 - val_loss: 0.2639 - val_accuracy: 0.9100\n",
      "Epoch 125/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2333 - accuracy: 0.9162 - val_loss: 0.2356 - val_accuracy: 0.9200\n",
      "Epoch 126/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2417 - accuracy: 0.9137 - val_loss: 0.2768 - val_accuracy: 0.9050\n",
      "Epoch 127/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2518 - accuracy: 0.9079 - val_loss: 0.2614 - val_accuracy: 0.9200\n",
      "Epoch 128/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9167 - val_loss: 0.2567 - val_accuracy: 0.9200\n",
      "Epoch 129/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2272 - accuracy: 0.9208 - val_loss: 0.2389 - val_accuracy: 0.9200\n",
      "Epoch 130/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2334 - accuracy: 0.9187 - val_loss: 0.2753 - val_accuracy: 0.8900\n",
      "Epoch 131/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2281 - accuracy: 0.9204 - val_loss: 0.2668 - val_accuracy: 0.9150\n",
      "Epoch 132/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2489 - accuracy: 0.9079 - val_loss: 0.2713 - val_accuracy: 0.9050\n",
      "Epoch 133/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2424 - accuracy: 0.9121 - val_loss: 0.2870 - val_accuracy: 0.8850\n",
      "Epoch 134/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2325 - accuracy: 0.9112 - val_loss: 0.2620 - val_accuracy: 0.9150\n",
      "Epoch 135/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9187 - val_loss: 0.2467 - val_accuracy: 0.9250\n",
      "Epoch 136/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2400 - accuracy: 0.9129 - val_loss: 0.2734 - val_accuracy: 0.9150\n",
      "Epoch 137/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2348 - accuracy: 0.9175 - val_loss: 0.2608 - val_accuracy: 0.9100\n",
      "Epoch 138/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2408 - accuracy: 0.9142 - val_loss: 0.2808 - val_accuracy: 0.9050\n",
      "Epoch 139/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2252 - accuracy: 0.9179 - val_loss: 0.2805 - val_accuracy: 0.9100\n",
      "Epoch 140/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2201 - accuracy: 0.9229 - val_loss: 0.2372 - val_accuracy: 0.9300\n",
      "Epoch 141/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2168 - accuracy: 0.9225 - val_loss: 0.2647 - val_accuracy: 0.9200\n",
      "Epoch 142/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2254 - accuracy: 0.9154 - val_loss: 0.2777 - val_accuracy: 0.9100\n",
      "Epoch 143/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2253 - accuracy: 0.9192 - val_loss: 0.2643 - val_accuracy: 0.9250\n",
      "Epoch 144/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2329 - accuracy: 0.9150 - val_loss: 0.2772 - val_accuracy: 0.9150\n",
      "Epoch 145/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2337 - accuracy: 0.9167 - val_loss: 0.2642 - val_accuracy: 0.9050\n",
      "Epoch 146/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2515 - accuracy: 0.9096 - val_loss: 0.2549 - val_accuracy: 0.9150\n",
      "Epoch 147/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2322 - accuracy: 0.9217 - val_loss: 0.2459 - val_accuracy: 0.9100\n",
      "Epoch 148/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2270 - accuracy: 0.9187 - val_loss: 0.2747 - val_accuracy: 0.9100\n",
      "Epoch 149/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2464 - accuracy: 0.9187 - val_loss: 0.2778 - val_accuracy: 0.9100\n",
      "Epoch 150/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2323 - accuracy: 0.9192 - val_loss: 0.2500 - val_accuracy: 0.9050\n",
      "Epoch 151/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2160 - accuracy: 0.9225 - val_loss: 0.2316 - val_accuracy: 0.9300\n",
      "Epoch 152/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2210 - accuracy: 0.9200 - val_loss: 0.2403 - val_accuracy: 0.9150\n",
      "Epoch 153/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2144 - accuracy: 0.9254 - val_loss: 0.2641 - val_accuracy: 0.8850\n",
      "Epoch 154/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2267 - accuracy: 0.9150 - val_loss: 0.2606 - val_accuracy: 0.9200\n",
      "Epoch 155/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2214 - accuracy: 0.9271 - val_loss: 0.2517 - val_accuracy: 0.9100\n",
      "Epoch 156/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2291 - accuracy: 0.9142 - val_loss: 0.2650 - val_accuracy: 0.9250\n",
      "Epoch 157/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2439 - accuracy: 0.9121 - val_loss: 0.2584 - val_accuracy: 0.9050\n",
      "Epoch 158/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2120 - accuracy: 0.9229 - val_loss: 0.2432 - val_accuracy: 0.9200\n",
      "Epoch 159/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2274 - accuracy: 0.9179 - val_loss: 0.2497 - val_accuracy: 0.9150\n",
      "Epoch 160/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2290 - accuracy: 0.9162 - val_loss: 0.2556 - val_accuracy: 0.9200\n",
      "Epoch 161/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9267 - val_loss: 0.2642 - val_accuracy: 0.9050\n",
      "Epoch 162/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2187 - accuracy: 0.9221 - val_loss: 0.2504 - val_accuracy: 0.9000\n",
      "Epoch 163/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2170 - accuracy: 0.9225 - val_loss: 0.2789 - val_accuracy: 0.9150\n",
      "Epoch 164/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2168 - accuracy: 0.9229 - val_loss: 0.2465 - val_accuracy: 0.9000\n",
      "Epoch 165/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2183 - accuracy: 0.9204 - val_loss: 0.2385 - val_accuracy: 0.9150\n",
      "Epoch 166/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2192 - accuracy: 0.9217 - val_loss: 0.2433 - val_accuracy: 0.9200\n",
      "Epoch 167/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2043 - accuracy: 0.9287 - val_loss: 0.2414 - val_accuracy: 0.9250\n",
      "Epoch 168/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2085 - accuracy: 0.9283 - val_loss: 0.2307 - val_accuracy: 0.9150\n",
      "Epoch 169/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2158 - accuracy: 0.9233 - val_loss: 0.2639 - val_accuracy: 0.9050\n",
      "Epoch 170/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9246 - val_loss: 0.2749 - val_accuracy: 0.9200\n",
      "Epoch 171/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2244 - accuracy: 0.9204 - val_loss: 0.2634 - val_accuracy: 0.9000\n",
      "Epoch 172/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2025 - accuracy: 0.9262 - val_loss: 0.2399 - val_accuracy: 0.9250\n",
      "Epoch 173/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2186 - accuracy: 0.9229 - val_loss: 0.2359 - val_accuracy: 0.9200\n",
      "Epoch 174/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9208 - val_loss: 0.2601 - val_accuracy: 0.9100\n",
      "Epoch 175/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2168 - accuracy: 0.9212 - val_loss: 0.2482 - val_accuracy: 0.9100\n",
      "Epoch 176/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2166 - accuracy: 0.9242 - val_loss: 0.2324 - val_accuracy: 0.9350\n",
      "Epoch 177/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9254 - val_loss: 0.2679 - val_accuracy: 0.9050\n",
      "Epoch 178/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2248 - accuracy: 0.9158 - val_loss: 0.2459 - val_accuracy: 0.9050\n",
      "Epoch 179/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2235 - accuracy: 0.9154 - val_loss: 0.2465 - val_accuracy: 0.9050\n",
      "Epoch 180/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2097 - accuracy: 0.9283 - val_loss: 0.2472 - val_accuracy: 0.9250\n",
      "Epoch 181/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2161 - accuracy: 0.9217 - val_loss: 0.2500 - val_accuracy: 0.9050\n",
      "Epoch 182/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2205 - accuracy: 0.9246 - val_loss: 0.2443 - val_accuracy: 0.9100\n",
      "Epoch 183/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2190 - accuracy: 0.9196 - val_loss: 0.2579 - val_accuracy: 0.9100\n",
      "Epoch 184/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2172 - accuracy: 0.9242 - val_loss: 0.2510 - val_accuracy: 0.9150\n",
      "Epoch 185/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9137 - val_loss: 0.2537 - val_accuracy: 0.9200\n",
      "Epoch 186/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9292 - val_loss: 0.2615 - val_accuracy: 0.9000\n",
      "Epoch 187/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2134 - accuracy: 0.9229 - val_loss: 0.2565 - val_accuracy: 0.9100\n",
      "Epoch 188/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2116 - accuracy: 0.9287 - val_loss: 0.2363 - val_accuracy: 0.9150\n",
      "Epoch 189/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2223 - accuracy: 0.9229 - val_loss: 0.2767 - val_accuracy: 0.9100\n",
      "Epoch 190/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.2239 - accuracy: 0.9242 - val_loss: 0.2535 - val_accuracy: 0.9150\n",
      "Epoch 191/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2193 - accuracy: 0.9192 - val_loss: 0.2328 - val_accuracy: 0.9200\n",
      "Epoch 192/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2030 - accuracy: 0.9308 - val_loss: 0.2328 - val_accuracy: 0.9150\n",
      "Epoch 193/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2131 - accuracy: 0.9242 - val_loss: 0.2551 - val_accuracy: 0.9250\n",
      "Epoch 194/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.2077 - accuracy: 0.9271 - val_loss: 0.2187 - val_accuracy: 0.9350\n",
      "Epoch 195/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1994 - accuracy: 0.9262 - val_loss: 0.2676 - val_accuracy: 0.9000\n",
      "Epoch 196/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2217 - accuracy: 0.9187 - val_loss: 0.2435 - val_accuracy: 0.9150\n",
      "Epoch 197/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2226 - accuracy: 0.9179 - val_loss: 0.2618 - val_accuracy: 0.9050\n",
      "Epoch 198/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2025 - accuracy: 0.9317 - val_loss: 0.2453 - val_accuracy: 0.9150\n",
      "Epoch 199/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2089 - accuracy: 0.9246 - val_loss: 0.2373 - val_accuracy: 0.9250\n",
      "Epoch 200/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9287 - val_loss: 0.2641 - val_accuracy: 0.9050\n",
      "Epoch 201/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2130 - accuracy: 0.9208 - val_loss: 0.2687 - val_accuracy: 0.9000\n",
      "Epoch 202/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.2132 - accuracy: 0.9212 - val_loss: 0.2617 - val_accuracy: 0.8950\n",
      "Epoch 203/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2109 - accuracy: 0.9275 - val_loss: 0.2631 - val_accuracy: 0.9150\n",
      "Epoch 204/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2022 - accuracy: 0.9329 - val_loss: 0.2401 - val_accuracy: 0.9100\n",
      "Epoch 205/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2211 - accuracy: 0.9229 - val_loss: 0.2504 - val_accuracy: 0.9000\n",
      "Epoch 206/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.2104 - accuracy: 0.9271 - val_loss: 0.2370 - val_accuracy: 0.9200\n",
      "Epoch 207/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2050 - accuracy: 0.9279 - val_loss: 0.2574 - val_accuracy: 0.9050\n",
      "Epoch 208/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2034 - accuracy: 0.9258 - val_loss: 0.2405 - val_accuracy: 0.9000\n",
      "Epoch 209/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.1996 - accuracy: 0.9242 - val_loss: 0.2613 - val_accuracy: 0.9050\n",
      "Epoch 210/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2094 - accuracy: 0.9300 - val_loss: 0.2328 - val_accuracy: 0.9250\n",
      "Epoch 211/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9225 - val_loss: 0.2485 - val_accuracy: 0.9100\n",
      "Epoch 212/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2033 - accuracy: 0.9246 - val_loss: 0.2313 - val_accuracy: 0.9150\n",
      "Epoch 213/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2246 - accuracy: 0.9192 - val_loss: 0.2279 - val_accuracy: 0.9150\n",
      "Epoch 214/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2170 - accuracy: 0.9246 - val_loss: 0.2346 - val_accuracy: 0.9200\n",
      "Epoch 215/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1946 - accuracy: 0.9304 - val_loss: 0.2514 - val_accuracy: 0.9050\n",
      "Epoch 216/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2020 - accuracy: 0.9312 - val_loss: 0.2509 - val_accuracy: 0.9150\n",
      "Epoch 217/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1918 - accuracy: 0.9362 - val_loss: 0.2294 - val_accuracy: 0.9250\n",
      "Epoch 218/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.2057 - accuracy: 0.9258 - val_loss: 0.2636 - val_accuracy: 0.9100\n",
      "Epoch 219/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9287 - val_loss: 0.2256 - val_accuracy: 0.9200\n",
      "Epoch 220/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2017 - accuracy: 0.9242 - val_loss: 0.2413 - val_accuracy: 0.9250\n",
      "Epoch 221/250\n",
      "75/75 [==============================] - ETA: 0s - loss: 0.2045 - accuracy: 0.92 - 0s 3ms/step - loss: 0.2053 - accuracy: 0.9237 - val_loss: 0.2331 - val_accuracy: 0.9100\n",
      "Epoch 222/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2098 - accuracy: 0.9242 - val_loss: 0.2386 - val_accuracy: 0.9100\n",
      "Epoch 223/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2011 - accuracy: 0.9271 - val_loss: 0.2479 - val_accuracy: 0.9100\n",
      "Epoch 224/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2037 - accuracy: 0.9321 - val_loss: 0.2510 - val_accuracy: 0.9100\n",
      "Epoch 225/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2123 - accuracy: 0.9225 - val_loss: 0.2416 - val_accuracy: 0.9250\n",
      "Epoch 226/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1925 - accuracy: 0.9317 - val_loss: 0.3026 - val_accuracy: 0.8950\n",
      "Epoch 227/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.1983 - accuracy: 0.9271 - val_loss: 0.2761 - val_accuracy: 0.9200\n",
      "Epoch 228/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1957 - accuracy: 0.9296 - val_loss: 0.2167 - val_accuracy: 0.9350\n",
      "Epoch 229/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2014 - accuracy: 0.9271 - val_loss: 0.2445 - val_accuracy: 0.9100\n",
      "Epoch 230/250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2009 - accuracy: 0.9296 - val_loss: 0.2711 - val_accuracy: 0.9150\n",
      "Epoch 231/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2144 - accuracy: 0.9246 - val_loss: 0.2552 - val_accuracy: 0.8950\n",
      "Epoch 232/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1957 - accuracy: 0.9350 - val_loss: 0.2495 - val_accuracy: 0.9100\n",
      "Epoch 233/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2093 - accuracy: 0.9233 - val_loss: 0.2482 - val_accuracy: 0.9000\n",
      "Epoch 234/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1952 - accuracy: 0.9292 - val_loss: 0.2762 - val_accuracy: 0.9200\n",
      "Epoch 235/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1927 - accuracy: 0.9267 - val_loss: 0.2275 - val_accuracy: 0.9300\n",
      "Epoch 236/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1925 - accuracy: 0.9304 - val_loss: 0.2746 - val_accuracy: 0.9000\n",
      "Epoch 237/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.1876 - accuracy: 0.9342 - val_loss: 0.2492 - val_accuracy: 0.9350\n",
      "Epoch 238/250\n",
      "75/75 [==============================] - 0s 6ms/step - loss: 0.1914 - accuracy: 0.9275 - val_loss: 0.2329 - val_accuracy: 0.9250\n",
      "Epoch 239/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9221 - val_loss: 0.2440 - val_accuracy: 0.9250\n",
      "Epoch 240/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2126 - accuracy: 0.9237 - val_loss: 0.2383 - val_accuracy: 0.9300\n",
      "Epoch 241/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2108 - accuracy: 0.9254 - val_loss: 0.2784 - val_accuracy: 0.9050\n",
      "Epoch 242/250\n",
      "75/75 [==============================] - 0s 4ms/step - loss: 0.2030 - accuracy: 0.9229 - val_loss: 0.2087 - val_accuracy: 0.9350\n",
      "Epoch 243/250\n",
      "75/75 [==============================] - 0s 5ms/step - loss: 0.1863 - accuracy: 0.9342 - val_loss: 0.2468 - val_accuracy: 0.9100\n",
      "Epoch 244/250\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1964 - accuracy: 0.9342 - val_loss: 0.2588 - val_accuracy: 0.9100\n",
      "Epoch 245/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2184 - accuracy: 0.9217 - val_loss: 0.2472 - val_accuracy: 0.9200\n",
      "Epoch 246/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1942 - accuracy: 0.9304 - val_loss: 0.2606 - val_accuracy: 0.8950\n",
      "Epoch 247/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9350 - val_loss: 0.2350 - val_accuracy: 0.9250\n",
      "Epoch 248/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1929 - accuracy: 0.9283 - val_loss: 0.2386 - val_accuracy: 0.9150\n",
      "Epoch 249/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.2027 - accuracy: 0.9275 - val_loss: 0.2476 - val_accuracy: 0.9250\n",
      "Epoch 250/250\n",
      "75/75 [==============================] - 0s 2ms/step - loss: 0.1842 - accuracy: 0.9358 - val_loss: 0.2325 - val_accuracy: 0.9150\n"
     ]
    }
   ],
   "source": [
    "#train subject model\n",
    "history = History()\n",
    "\n",
    "train_position = model_position.fit(train_data, train_position, validation_data = (val_data, val_position), epochs=250,  callbacks = [history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-bdb27374c9ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Training accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"--\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlinewidth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Validation accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy Curves'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'acc'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1008x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(history.history['accuracy'],'black', linewidth = 3.0,  label = \"Training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'],'black', ls = \"--\",linewidth = 3.0, label = \"Validation accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "\n",
    "model_position.evaluate(test_data_p, test_position)\n",
    "\n",
    "predictions_position = model_position.predict(test_data_p)\n",
    "predictions_position = predictions_position.argmax(axis=-1)\n",
    "print(predictions_position[:10])\n",
    "\n",
    "test_labels = df.iloc[test_split:, num_col-1:num_col]\n",
    "test_labels.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_position = model_position.predict(test_data_p)\n",
    "predictions_position = predictions_position.argmax(axis=-1)\n",
    "\n",
    "labels = ['supine', 'right', 'left']\n",
    "test_labels_p = df.iloc[test_split:, num_col-1:num_col]\n",
    "cm = confusion_matrix(test_labels_p, predictions_position, [0,1,2])\n",
    "\n",
    "f,ax= plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, ax = ax, linewidths=1, fmt = 'd', cmap = \"Blues\"); #annot=True to annotate cells\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Position'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset in train, val and test\n",
    "num_col = len(df.columns)\n",
    "train_split = 2400\n",
    "test_split = 2600\n",
    "\n",
    "train_data = df.iloc[0:train_split, [2,5,8]]\n",
    "val_data = df.iloc[train_split:test_split, [2,5,8]]\n",
    "\n",
    "train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "test_data_s = df.iloc[test_split:,[2,5,8]]\n",
    "test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "test_data_s = np.array(test_data_s)\n",
    "test_subject = np.array(test_subject)\n",
    "test_position = np.array(test_position)\n",
    "test_subject = to_categorical(test_subject)\n",
    "test_position = to_categorical(test_position)\n",
    "\n",
    "while test_subject.shape[1] != 13:\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    train_data = df.iloc[0:train_split, [2,5,8]]\n",
    "    val_data = df.iloc[train_split:test_split, [2,5,8]]\n",
    "\n",
    "    train_subject = df.iloc[0:train_split, num_col-2:num_col-1]\n",
    "    train_position = df.iloc[0:train_split, num_col-1:num_col]\n",
    "    val_subject = df.iloc[train_split:test_split, num_col-2:num_col-1]\n",
    "    val_position = df.iloc[train_split:test_split, num_col-1:num_col]\n",
    "\n",
    "    test_data_s = df.iloc[test_split:,[2,5,8]]\n",
    "    test_subject = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "    test_position = df.iloc[test_split:, num_col-1:num_col]\n",
    "\n",
    "    test_data_s = np.array(test_data_s)\n",
    "    test_subject = np.array(test_subject)\n",
    "    test_position = np.array(test_position)\n",
    "    test_subject = to_categorical(test_subject)\n",
    "    test_position = to_categorical(test_position)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to numpy array\n",
    "train_data = np.array(train_data)\n",
    "val_data = np.array(val_data)\n",
    "\n",
    "train_subject = np.array(train_subject)\n",
    "train_position = np.array(train_position)\n",
    "val_subject = np.array(val_subject)\n",
    "val_position = np.array(val_position)\n",
    "\n",
    "print(train_subject.shape)\n",
    "print(train_position.shape)\n",
    "print(val_data.shape)\n",
    "print(val_subject.shape)\n",
    "print(val_position.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize data and build categorical labels\n",
    "train_subject = to_categorical(train_subject, 13)\n",
    "train_position = to_categorical(train_position, 3)\n",
    "val_subject = to_categorical(val_subject, 13)\n",
    "val_position = to_categorical(val_position, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model subject\n",
    "model_subject = Sequential()\n",
    "model_subject.add(Dense(128, input_dim= 3, activation='relu'))\n",
    "model_subject.add(Flatten())\n",
    "model_subject.add(Dropout(0.4))\n",
    "model_subject.add(Dense(64, activation='relu'))\n",
    "model_subject.add(Dense(32, activation='relu'))\n",
    "model_subject.add(Dense(32, activation='relu'))\n",
    "model_subject.add(Dense(16, activation='relu'))\n",
    "model_subject.add(Dense(9, activation='relu'))\n",
    "\n",
    "model_subject.add(Dense(13, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimiser = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "model_subject.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_subject.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train position model\n",
    "history = History()\n",
    "\n",
    "train_subject = model_subject.fit(train_data, train_subject, validation_data = (val_data, val_subject), epochs=1000, batch_size = 8, callbacks = [history])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14,10))\n",
    "plt.plot(history.history['accuracy'],'black', linewidth = 3.0,  label = \"Training accuracy\")\n",
    "plt.plot(history.history['val_accuracy'],'black', ls = \"--\",linewidth = 3.0, label = \"Validation accuracy\")\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.title('Accuracy Curves')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "model_subject.evaluate(test_data_s, test_subject)\n",
    "\n",
    "predictions_subject = model_subject.predict(test_data_s)\n",
    "predictions_subject = predictions_subject.argmax(axis=-1)\n",
    "print(predictions_subject[:10])\n",
    "\n",
    "test_labels = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "test_labels.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_subject = model_subject.predict(test_data_s)\n",
    "predictions_subject = predictions_subject.argmax(axis=-1)\n",
    "\n",
    "labels = ['S1', 'S2', 'S3', 'S4', 'S5', 'S6','S7', 'S8', 'S9','S10', 'S11', 'S12','S13']\n",
    "test_labels_s = df.iloc[test_split:, num_col-2:num_col-1]\n",
    "cm = confusion_matrix(test_labels_s, predictions_subject, [0,1,2,3,4,5,6,7,8,9,10,11,12])\n",
    "\n",
    "f,ax= plt.subplots(figsize=(10,10))\n",
    "sns.heatmap(cm, annot=True, ax = ax, linewidths=1, fmt = 'd', cmap=\"Greens\"); #annot=True to annotate cells\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
    "ax.set_title('Confusion Matrix Subject'); \n",
    "ax.xaxis.set_ticklabels(labels); ax.yaxis.set_ticklabels(labels);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save model and architecture to single file\n",
    "model_subject.save(\"model_subject_2.h5\")\n",
    "model_position.save(\"model_position_2.h5\")\n",
    "\n",
    "with open('test_2.pkl', 'wb') as f: \n",
    "    pickle.dump([test_data_s, test_data_p, test_subject, test_position, test_labels_s, test_labels_p], f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
